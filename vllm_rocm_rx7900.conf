# vLLM ROCm Server Configuration File
# =====================================
# Configuration for AMD GPUs with ROCm
# Updated for AMD's rocm/vllm Docker images
# Place at: /opt/inference/config/vllm_rocm.conf

# Model Configuration
# -------------------
MODEL_ID="NousResearch/Hermes-3-Llama-3.1-8B"
# MODEL_ID="NousResearch/Hermes-4-14B"  # Requires more memory
MODEL_LENGTH="8192"  # Adjust based on your model

# GPU Configuration (AMD-specific)
# ---------------------------------
GPU_MEMORY_UTILIZATION="0.90"
TENSOR_PARALLEL_SIZE="1"  # Set to 2 for dual GPU with larger models
PIPELINE_PARALLEL_SIZE="1"

# AMD GPU Selection
# Use HIP_VISIBLE_DEVICES to select specific GPUs
# Examples:
#   "0"     - Use first GPU only
#   "0,1"   - Use both GPUs
#   "1"     - Use second GPU only
HIP_VISIBLE_DEVICES="0"  # Change to "0,1" for dual GPU

# ROCm Compatibility
# ------------------
ROCM_VERSION="6.4"  # Your ROCm version (6.2, 6.4, etc.)

# For RX 7900 XTX, you might need this for older images:
# HSA_OVERRIDE_GFX_VERSION="11.0.0"

# Server Configuration
# --------------------
VLLM_PORT="8081"
VLLM_HOST="0.0.0.0"
VLLM_LOGGING_LEVEL="INFO"
CONTAINER_NAME="vllm-rocm-server"
RESTART_POLICY="unless-stopped"

# Docker Image Configuration
# --------------------------
# AMD maintains vLLM images at rocm/vllm (not vllm/vllm-rocm)
IMAGE_REGISTRY="rocm"
IMAGE_NAME="vllm"

# Choose your image tag based on ROCm version:
# For ROCm 6.4.x:
IMAGE_TAG="rocm6.4.1_vllm_0.10.1_20250909"

# For ROCm 6.2.x:
# IMAGE_TAG="rocm6.2_ubuntu22.04_py3.10_vllm_0.6.3"

# For ROCm 6.1.x:
# IMAGE_TAG="rocm6.1.2_ubuntu22.04_py3.10_vllm"

# Browse available tags at:
# https://hub.docker.com/r/rocm/vllm/tags

# vLLM Features
# -------------
TOOL_CALL_PARSER="hermes"
ENABLE_AUTO_TOOL_CHOICE="true"
ENABLE_CHUNKED_PREFILL="false"
ENABLE_PREFIX_CACHING="false"

# Performance Tuning (AMD-optimized)
# -----------------------------------
MAX_NUM_SEQS="128"  # Increase for better throughput
BLOCK_SIZE="16"
NUM_SCHEDULER_STEPS="1"

# KV cache data type
# For RX 7900 XTX with 24GB VRAM, "auto" usually works well
# Use "fp16" to save memory for larger models
KV_CACHE_DTYPE="auto"

# Optional: Set max batched tokens for better batching
# MAX_NUM_BATCHED_TOKENS="8192"

# API Settings
# ------------
# API_KEY=""  # Optional API key for security
RESPONSE_ROLE="assistant"

# Cache Directories
# -----------------
HF_CACHE="/home/shaun/.cache/huggingface"
VLLM_CACHE="/opt/inference/cache"
DOWNLOAD_DIR="/opt/inference/downloads"

# Logging
# -------
LOG_DIR="/opt/inference/logs"

# ============================================
# Configuration Profiles for RX 7900 XTX
# ============================================

# Profile 1: Single GPU - 8B Model (Recommended for start)
# ---------------------------------------------------------
# MODEL_ID="NousResearch/Hermes-3-Llama-3.1-8B"
# MODEL_LENGTH="8192"
# HIP_VISIBLE_DEVICES="0"
# TENSOR_PARALLEL_SIZE="1"
# GPU_MEMORY_UTILIZATION="0.90"
# MAX_NUM_SEQS="128"

# Profile 2: Dual GPU - 14B Model
# --------------------------------
# MODEL_ID="NousResearch/Hermes-4-14B"
# MODEL_LENGTH="32768"
# HIP_VISIBLE_DEVICES="0,1"
# TENSOR_PARALLEL_SIZE="2"
# GPU_MEMORY_UTILIZATION="0.88"
# MAX_NUM_SEQS="64"

# Profile 3: Single GPU - Maximum Throughput
# -------------------------------------------
# HIP_VISIBLE_DEVICES="0"
# TENSOR_PARALLEL_SIZE="1"
# GPU_MEMORY_UTILIZATION="0.95"
# MAX_NUM_SEQS="256"
# MAX_NUM_BATCHED_TOKENS="16384"
# ENABLE_PREFIX_CACHING="true"

# Profile 4: Dual GPU - Large Context
# ------------------------------------
# HIP_VISIBLE_DEVICES="0,1"
# TENSOR_PARALLEL_SIZE="2"
# MODEL_LENGTH="32768"
# GPU_MEMORY_UTILIZATION="0.85"
# ENABLE_CHUNKED_PREFILL="true"

# ============================================
# Troubleshooting for RX 7900 XTX
# ============================================

# If you get "GPU not supported" errors:
# ---------------------------------------
# Uncomment this line:
# HSA_OVERRIDE_GFX_VERSION="11.0.0"

# If you get out of memory errors:
# ---------------------------------
# 1. Reduce GPU_MEMORY_UTILIZATION to 0.80
# 2. Reduce MAX_NUM_SEQS to 32
# 3. Use KV_CACHE_DTYPE="fp16"
# 4. Reduce MODEL_LENGTH if possible

# If inference is slow:
# ---------------------
# 1. Enable ENABLE_PREFIX_CACHING="true"
# 2. Increase MAX_NUM_SEQS for better batching
# 3. Check GPU clock speeds: rocm-smi --showclocks
# 4. Ensure power profile is set to performance:
#    sudo rocm-smi --setperflevel high

# Monitor GPU during inference:
# -----------------------------
# rocm-smi --showuse
# rocm-smi --showtemp
# rocm-smi --showpower
# watch -n 1 rocm-smi
